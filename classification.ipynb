{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import shap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your dataset (replace with the correct path to your Excel file)\n",
    "df = pd.read_excel('/home/sruthi/conference/filtered_data_DLS_diameter.xlsx')\n",
    "\n",
    "\n",
    "# Exclude Abs_max and FW_80M features\n",
    "features_to_exclude = [ 'Name','DLS Diameter [nm]','Standard Deviation DLS [nm]']\n",
    "X = df.drop(columns=features_to_exclude)\n",
    "\n",
    "# Use DLS_diameter as the label\n",
    "y = df['DLS Diameter [nm]']\n",
    "\n",
    "# Convert DLS_diameter to binary classification\n",
    "y = (y >= 50).astype(int)  # 1 for >= 50, 0 for < 50\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using lazypredict to get several classifier model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lazypredict.Supervised import LazyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LazyClassifier\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Display the results\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing classification but with LGBM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset (replace with the correct path to your Excel file)\n",
    "df = pd.read_excel('/home/sruthi/conference/filtered_data_DLS_diameter.xlsx')\n",
    "\n",
    "\n",
    "# Exclude Abs_max and FW_80M features\n",
    "features_to_exclude = [ 'Name','DLS Diameter [nm]','Standard Deviation DLS [nm]']\n",
    "X = df.drop(columns=features_to_exclude)\n",
    "\n",
    "# Use DLS_diameter as the label\n",
    "y = df['DLS Diameter [nm]']\n",
    "\n",
    "# Convert DLS_diameter to binary classification\n",
    "y = (y >= 50).astype(int)  # 1 for >= 50, 0 for < 50\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Initialize and train the LGBMClassifier\n",
    "lgbm_model = lgbm_model = LGBMClassifier(random_state=9999)\n",
    "lgbm_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "# Evaluation Metrics for Training Data\n",
    "y_train_pred = lgbm_model.predict(X_train)\n",
    "y_train_pred_proba = lgbm_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Calculate metrics for training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "# Compute confusion matrix components for training data\n",
    "train_conf_matrix = confusion_matrix(y_train, y_train_pred)\n",
    "tn_train, fp_train, fn_train, tp_train = train_conf_matrix.ravel()\n",
    "\n",
    "# Calculate specificity for training data\n",
    "train_specificity = tn_train / (tn_train + fp_train)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"Training Data Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Precision: {train_precision:.2f}\")\n",
    "print(f\"Recall: {train_recall:.2f}\")\n",
    "print(f\"F1-Score: {train_f1:.2f}\")\n",
    "print(f\"ROC-AUC Score: {train_roc_auc:.2f}\")\n",
    "print(f\"Specificity: {train_specificity:.2f}\")\n",
    "\n",
    "# Evaluation Metrics for Test Data\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "y_pred_proba = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics for test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Compute confusion matrix components for test data\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate specificity for test data\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print test metrics\n",
    "print(\"\\nTest Data Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "# Compare Training and Test Metrics\n",
    "print(\"\\nComparison of Training vs Test Metrics:\")\n",
    "print(f\"Accuracy - Train: {train_accuracy:.2f}, Test: {accuracy:.2f}\")\n",
    "print(f\"Precision - Train: {train_precision:.2f}, Test: {precision:.2f}\")\n",
    "print(f\"Recall - Train: {train_recall:.2f}, Test: {recall:.2f}\")\n",
    "print(f\"F1-Score - Train: {train_f1:.2f}, Test: {f1:.2f}\")\n",
    "print(f\"ROC-AUC - Train: {train_roc_auc:.2f}, Test: {roc_auc:.2f}\")\n",
    "print(f\"Specificity - Train: {train_specificity:.2f}, Test: {specificity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance from LGBMclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming lgbm_model is already trained, and X is a NumPy array, but we need original feature names.\n",
    "# For example, use the original DataFrame `df` or `X_train` that you used before scaling\n",
    "feature_names = df.drop(columns=['Name', 'DLS Diameter [nm]', 'Standard Deviation DLS [nm]']).columns\n",
    "\n",
    "# Get feature importance based on split count\n",
    "importance_split = lgbm_model.feature_importances_\n",
    "\n",
    "# Combine into a DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance (Split)': importance_split\n",
    "})\n",
    "\n",
    "# Sort and display feature importance\n",
    "importance_df = importance_df.sort_values(by='Importance (Split)', ascending=False)\n",
    "print(importance_df)\n",
    "\n",
    "# Plot feature importance (Split)\n",
    "lgb.plot_importance(lgbm_model, importance_type='split', max_num_features=10, figsize=(10, 6))\n",
    "plt.title(\"Feature Importance - Split Count\")\n",
    "plt.show()\n",
    "\n",
    "# Plot feature importance (Gain)\n",
    "lgb.plot_importance(lgbm_model, importance_type='gain', max_num_features=10, figsize=(10, 6))\n",
    "plt.title(\"Feature Importance - Gain\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and their importances\n",
    "feature_names = df.drop(columns=['Name', 'DLS Diameter [nm]', 'Standard Deviation DLS [nm]']).columns\n",
    "importance_split = lgbm_model.feature_importances_\n",
    "\n",
    "# Combine into a DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance (Split)': importance_split\n",
    "}).sort_values(by='Importance (Split)', ascending=False)\n",
    "\n",
    "# Display the sorted importance values\n",
    "print(importance_df)\n",
    "\n",
    "# Custom plot for split importance with split values displayed\n",
    "plt.figure(figsize=(10, 6))\n",
    "y_values = importance_df['Feature'][:20][::-1]\n",
    "x_values = importance_df['Importance (Split)'][:20][::-1]\n",
    "plt.barh(y_values, x_values, color='skyblue')\n",
    "\n",
    "# Annotate the bars with their importance values\n",
    "for i, (value, name) in enumerate(zip(x_values, y_values)):\n",
    "    plt.text(value + 2, i, str(value), va='center', fontsize=9, color='black')\n",
    "\n",
    "plt.xlabel(\"Importance (Split)\")\n",
    "plt.title(\"Feature Importance - Split Count (with Values)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the split importance plot as a JPG\n",
    "plt.savefig(\"feature_importance_split_with_values.jpg\", format=\"jpg\")\n",
    "plt.show()\n",
    "# Save the gain importance plot as a JPG\n",
    "plt.savefig(\"feature_importance_gain_with_values.jpg\", format=\"jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 6 features from feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random state for reproducibility\n",
    "rs = 99\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_excel('/home/sruthi/conference/filtered_data_DLS_diameter.xlsx')\n",
    "\n",
    "# Exclude irrelevant features from the dataset\n",
    "features_to_exclude = ['Name', 'DLS Diameter [nm]', 'Standard Deviation DLS [nm]']\n",
    "X_df = df.drop(columns=features_to_exclude)\n",
    "\n",
    "# Strip leading and trailing spaces from column names\n",
    "X_df.columns = X_df.columns.str.strip()\n",
    "\n",
    "# Print out the columns to check for any discrepancies\n",
    "print(\"Columns in the dataset:\", X_df.columns)\n",
    "\n",
    "# Use DLS_diameter as the label and convert it to binary classification (1 if >= 50, else 0)\n",
    "y = df['DLS Diameter [nm]']\n",
    "y = (y >= 50).astype(int)  # 1 for >= 50, 0 for < 50\n",
    "\n",
    "# Top 6 features based on feature importance\n",
    "top_6_features = [\n",
    "    '$n_\\mathrm{t}$(AA)/n(Au)', 'd$n$(AA)/d$t$ [µmol/min]', '$V$(Seeds) [µL]',\n",
    "    '$c_\\mathrm{f}$(AA) [mM]', 'd$V$(Au)/d$t$ [µL/min]', 'd$n$(Au)/d$t$ [µmol/min]'\n",
    "]\n",
    "\n",
    "# Ensure all features are present in the DataFrame\n",
    "missing_features = [feature for feature in top_6_features if feature not in X_df.columns]\n",
    "if missing_features:\n",
    "    print(\"Missing features:\", missing_features)\n",
    "else:\n",
    "    # Select only the top 6 features\n",
    "    X_top_6_df = X_df[top_6_features]\n",
    "\n",
    "    # Normalize the features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_top_6_scaled = scaler.fit_transform(X_top_6_df)\n",
    "\n",
    "    # Split into training and testing sets with random_state=rs\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_top_6_scaled, y, test_size=0.3, random_state=rs)\n",
    "\n",
    "    # Initialize and train the LGBMClassifier\n",
    "    lgbm_model = LGBMClassifier(random_state=rs)\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluation Metrics for Training Data\n",
    "    y_train_pred = lgbm_model.predict(X_train)\n",
    "    y_train_pred_proba = lgbm_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "    # Calculate metrics for training data\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "    # Compute confusion matrix components for training data\n",
    "    train_conf_matrix = confusion_matrix(y_train, y_train_pred)\n",
    "    tn_train, fp_train, fn_train, tp_train = train_conf_matrix.ravel()\n",
    "\n",
    "    # Calculate specificity for training data\n",
    "    train_specificity = tn_train / (tn_train + fp_train)\n",
    "\n",
    "    # Print training metrics\n",
    "    print(\"Training Data Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {train_accuracy:.2f}\")\n",
    "    print(f\"Precision: {train_precision:.2f}\")\n",
    "    print(f\"Recall: {train_recall:.2f}\")\n",
    "    print(f\"F1-Score: {train_f1:.2f}\")\n",
    "    print(f\"ROC-AUC Score: {train_roc_auc:.2f}\")\n",
    "    print(f\"Specificity: {train_specificity:.2f}\")\n",
    "\n",
    "    # Evaluation Metrics for Test Data\n",
    "    y_pred = lgbm_model.predict(X_test)\n",
    "    y_pred_proba = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics for test data\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Compute confusion matrix components for test data\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    # Calculate specificity for test data\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Print test metrics\n",
    "    print(\"\\nTest Data Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
    "    print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "    # Compare Training and Test Metrics\n",
    "    print(\"\\nComparison of Training vs Test Metrics:\")\n",
    "    print(f\"Accuracy - Train: {train_accuracy:.2f}, Test: {accuracy:.2f}\")\n",
    "    print(f\"Precision - Train: {train_precision:.2f}, Test: {precision:.2f}\")\n",
    "    print(f\"Recall - Train: {train_recall:.2f}, Test: {recall:.2f}\")\n",
    "    print(f\"F1-Score - Train: {train_f1:.2f}, Test: {f1:.2f}\")\n",
    "    print(f\"ROC-AUC - Train: {train_roc_auc:.2f}, Test: {roc_auc:.2f}\")\n",
    "    print(f\"Specificity - Train: {train_specificity:.2f}, Test: {specificity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOP 3 features from feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 99\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_excel('/home/sruthi/conference/filtered_data_DLS_diameter.xlsx')\n",
    "\n",
    "# Exclude features from the dataset\n",
    "features_to_exclude = ['Name', 'DLS Diameter [nm]', 'Standard Deviation DLS [nm]']\n",
    "X_df = df.drop(columns=features_to_exclude)\n",
    "\n",
    "# Strip leading and trailing spaces from column names\n",
    "X_df.columns = X_df.columns.str.strip()\n",
    "\n",
    "# Print out the columns to check for any discrepancies\n",
    "print(\"Columns in the dataset:\", X_df.columns)\n",
    "\n",
    "# Use DLS_diameter as the label and convert it to binary classification (1 if >= 50, else 0)\n",
    "y = df['DLS Diameter [nm]']\n",
    "y = (y >= 50).astype(int)  # 1 for >= 50, 0 for < 50\n",
    "\n",
    "# Top 6 features based on feature importance\n",
    "top_6_features = [\n",
    "    '$n_\\mathrm{t}$(AA)/n(Au)', 'd$n$(AA)/d$t$ [µmol/min]', '$V$(Seeds) [µL]'\n",
    "]\n",
    "\n",
    "# Ensure all features are present in the DataFrame\n",
    "missing_features = [feature for feature in top_6_features if feature not in X_df.columns]\n",
    "if missing_features:\n",
    "    print(\"Missing features:\", missing_features)\n",
    "else:\n",
    "    # Select only the top 6 features\n",
    "    X_top_6_df = X_df[top_6_features]\n",
    "\n",
    "    # Normalize the features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_top_6_scaled = scaler.fit_transform(X_top_6_df)\n",
    "\n",
    "    # Split into training and testing sets with random_state=10 for reproducibility\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_top_6_scaled, y, test_size=0.3, random_state=rs)\n",
    "\n",
    "    # Initialize and train the LGBMClassifier\n",
    "    lgbm_model = LGBMClassifier(random_state=rs)\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluation Metrics for Training Data\n",
    "    y_train_pred = lgbm_model.predict(X_train)\n",
    "    y_train_pred_proba = lgbm_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "    # Calculate metrics for training data\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "    # Compute confusion matrix components for training data\n",
    "    train_conf_matrix = confusion_matrix(y_train, y_train_pred)\n",
    "    tn_train, fp_train, fn_train, tp_train = train_conf_matrix.ravel()\n",
    "\n",
    "    # Calculate specificity for training data\n",
    "    train_specificity = tn_train / (tn_train + fp_train)\n",
    "\n",
    "    # Print training metrics\n",
    "    print(\"Training Data Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {train_accuracy:.2f}\")\n",
    "    print(f\"Precision: {train_precision:.2f}\")\n",
    "    print(f\"Recall: {train_recall:.2f}\")\n",
    "    print(f\"F1-Score: {train_f1:.2f}\")\n",
    "    print(f\"ROC-AUC Score: {train_roc_auc:.2f}\")\n",
    "    print(f\"Specificity: {train_specificity:.2f}\")\n",
    "\n",
    "    # Evaluation Metrics for Test Data\n",
    "    y_pred = lgbm_model.predict(X_test)\n",
    "    y_pred_proba = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics for test data\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Compute confusion matrix components for test data\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    # Calculate specificity for test data\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Print test metrics\n",
    "    print(\"\\nTest Data Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
    "    print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "    # Compare Training and Test Metrics\n",
    "    print(\"\\nComparison of Training vs Test Metrics:\")\n",
    "    print(f\"Accuracy - Train: {train_accuracy:.2f}, Test: {accuracy:.2f}\")\n",
    "    print(f\"Precision - Train: {train_precision:.2f}, Test: {precision:.2f}\")\n",
    "    print(f\"Recall - Train: {train_recall:.2f}, Test: {recall:.2f}\")\n",
    "    print(f\"F1-Score - Train: {train_f1:.2f}, Test: {f1:.2f}\")\n",
    "    print(f\"ROC-AUC - Train: {train_roc_auc:.2f}, Test: {roc_auc:.2f}\")\n",
    "    print(f\"Specificity - Train: {train_specificity:.2f}, Test: {specificity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis\n",
    "explainer = shap.Explainer(lgbm_model, X_train)\n",
    "shap_values = explainer(X_train)\n",
    "\n",
    "# SHAP summary plot with feature names from the dataset\n",
    "shap.summary_plot(shap_values, X_train, feature_names=df.drop(columns=features_to_exclude).columns)\n",
    "\n",
    "# Other SHAP plots\n",
    "shap.summary_plot(shap_values, X_train, feature_names=df.drop(columns=features_to_exclude).columns, plot_type='bar')\n",
    "#shap.summary_plot(shap_values, X_train, feature_names=df.drop(columns=features_to_exclude).columns, plot_type='dot')\n",
    "\n",
    "# Save the plots as JPG\n",
    "plt.savefig(\"feature_importance_split_with_values.jpg\", format=\"jpg\")\n",
    "plt.show()\n",
    "plt.savefig(\"feature_importance_gain_with_values.jpg\", format=\"jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top 5 features from SHAP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 99\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_excel('/home/sruthi/conference/filtered_data_DLS_diameter.xlsx')\n",
    "\n",
    "# Exclude features from the dataset\n",
    "features_to_exclude = ['Name', 'DLS Diameter [nm]', 'Standard Deviation DLS [nm]']\n",
    "X_df = df.drop(columns=features_to_exclude)\n",
    "\n",
    "# Strip leading and trailing spaces from column names\n",
    "X_df.columns = X_df.columns.str.strip()\n",
    "\n",
    "# Print out the columns to check for any discrepancies\n",
    "print(\"Columns in the dataset:\", X_df.columns)\n",
    "\n",
    "# Use DLS_diameter as the label and convert it to binary classification (1 if >= 50, else 0)\n",
    "y = df['DLS Diameter [nm]']\n",
    "y = (y >= 50).astype(int)  # 1 for >= 50, 0 for < 50\n",
    "\n",
    "# Top 5 features based on SHAP analysis\n",
    "top_5_features_SHAP = [\n",
    "    '$n_\\mathrm{t}$(AA)/n(Au)', 'd$n$(AA)/d$t$ [µmol/min]', '$V$(Seeds) [µL]',\n",
    "    '$c_\\mathrm{f}$(AA) [mM]', '$c_\\mathrm{e}$(AA) [mM]'\n",
    "]\n",
    "\n",
    "# Ensure all features are present in the DataFrame\n",
    "missing_features = [feature for feature in top_5_features_SHAP if feature not in X_df.columns]\n",
    "if missing_features:\n",
    "    print(\"Missing features:\", missing_features)\n",
    "else:\n",
    "    # Select only the top 5 features\n",
    "    X_top_5_df = X_df[top_5_features_SHAP]\n",
    "\n",
    "    # Normalize the features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_top_5_scaled = scaler.fit_transform(X_top_5_df)\n",
    "\n",
    "    # Split into training and testing sets with random_state=rs\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_top_5_scaled, y, test_size=0.3, random_state=rs)\n",
    "\n",
    "    # Initialize and train the LGBMClassifier\n",
    "    lgbm_model = LGBMClassifier(random_state=rs)\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluation Metrics for Training Data\n",
    "    y_train_pred = lgbm_model.predict(X_train)\n",
    "    y_train_pred_proba = lgbm_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "    # Calculate metrics for training data\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "    # Compute confusion matrix components for training data\n",
    "    train_conf_matrix = confusion_matrix(y_train, y_train_pred)\n",
    "    tn_train, fp_train, fn_train, tp_train = train_conf_matrix.ravel()\n",
    "\n",
    "    # Calculate specificity for training data\n",
    "    train_specificity = tn_train / (tn_train + fp_train)\n",
    "\n",
    "    # Print training metrics\n",
    "    print(\"Training Data Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {train_accuracy:.2f}\")\n",
    "    print(f\"Precision: {train_precision:.2f}\")\n",
    "    print(f\"Recall: {train_recall:.2f}\")\n",
    "    print(f\"F1-Score: {train_f1:.2f}\")\n",
    "    print(f\"ROC-AUC Score: {train_roc_auc:.2f}\")\n",
    "    print(f\"Specificity: {train_specificity:.2f}\")\n",
    "\n",
    "    # Evaluation Metrics for Test Data\n",
    "    y_pred = lgbm_model.predict(X_test)\n",
    "    y_pred_proba = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics for test data\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Compute confusion matrix components for test data\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    # Calculate specificity for test data\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Print test metrics\n",
    "    print(\"\\nTest Data Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
    "    print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "    # Compare Training and Test Metrics\n",
    "    print(\"\\nComparison of Training vs Test Metrics:\")\n",
    "    print(f\"Accuracy - Train: {train_accuracy:.2f}, Test: {accuracy:.2f}\")\n",
    "    print(f\"Precision - Train: {train_precision:.2f}, Test: {precision:.2f}\")\n",
    "    print(f\"Recall - Train: {train_recall:.2f}, Test: {recall:.2f}\")\n",
    "    print(f\"F1-Score - Train: {train_f1:.2f}, Test: {f1:.2f}\")\n",
    "    print(f\"ROC-AUC - Train: {train_roc_auc:.2f}, Test: {roc_auc:.2f}\")\n",
    "    print(f\"Specificity - Train: {train_specificity:.2f}, Test: {specificity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
